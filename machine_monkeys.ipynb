{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  50-007-machine-learning-summer-2024.zip\n","  inflating: sample_submission.csv   \n","  inflating: test.csv                \n","  inflating: test_tfidf_features.csv  \n","  inflating: train.csv               \n","  inflating: train_tfidf_features.csv  \n"]}],"source":["!unzip 50-007-machine-learning-summer-2024.zip\n","\n","# run this cell to extract the data (but cannot commit after because file exceeds 100MB)"]},{"cell_type":"markdown","metadata":{},"source":["Your implementation should have the following functions:\n","\n","-- sigmoid(z): A function that takes in a Real Number input and returns an output value between 0 and 1.\n","\n","-- loss(y, y_hat): A loss function that allows us to minimize and determine the optimal parameters. The function takes in the actual labels y and the predicted labels y_hat, and returns the overall training loss. Note that you should be using the Log Loss function taught in class.\n","\n","-- gradients(X, y, y_hat): The Gradient Descent Algorithm to find the optimal values of our parameters. The function takes in the training feature X, actual labels y and the predicted labels y_hat, and returns the partial derivative of the Loss function with respect to weights (w) and bias (db).\n","\n","-- train(X, y, bs, epochs, lr): The training function for your model.\n","\n","-- predict(X): The prediction function where you can apply your validation and test sets."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def loss(y, y_hat):\n","    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))\n","    return loss"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def gradients(X, y, y_hat):\n","    \n","    # X --> Input.\n","    # y --> true/target value.\n","    # y_hat --> hypothesis/predictions.\n","    # w --> weights (parameter).\n","    # b --> bias (parameter).\n","    \n","    # m-> number of training examples.\n","    m = X.shape[0]\n","    \n","    # Gradient of loss w.r.t weights.\n","    dw = (1/m)*np.dot(X.T, (y_hat - y))\n","    \n","    # Gradient of loss w.r.t bias.\n","    db = (1/m)*np.sum((y_hat - y)) \n","    \n","    return dw, db\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def plot_decision_boundary(X, w, b):\n","    \n","    # X --> Inputs\n","    # w --> weights\n","    # b --> bias\n","    \n","    # The Line is y=mx+c\n","    # So, Equate mx+c = w.X + b\n","    # Solving we find m and c\n","    x1 = [min(X[:,0]), max(X[:,0])]\n","    m = -w[0]/w[1]\n","    c = -b/w[1]\n","    x2 = m*x1 + c\n","    \n","    # Plotting\n","    fig = plt.figure(figsize=(10,8))\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"g^\")\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n","    plt.xlim([-2, 2])\n","    plt.ylim([0, 2.2])\n","    plt.xlabel(\"feature 1\")\n","    plt.ylabel(\"feature 2\")\n","    plt.title('Decision Boundary')\n","    plt.plot(x1, x2, 'y-')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def normalize(X):\n","    \n","    # X --> Input.\n","    \n","    # m-> number of training examples\n","    # n-> number of features \n","    m, n = X.shape\n","    \n","    return X"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def train(X, y, bs, epochs, lr):\n","    \n","    # X --> Input.\n","    # y --> true/target value.\n","    # bs --> Batch Size.\n","    # epochs --> Number of iterations.\n","    # lr --> Learning rate.\n","        \n","    # m-> number of training examples\n","    # n-> number of features \n","    m, n = X.shape\n","    \n","    # Initializing weights and bias to zeros.\n","    w = np.zeros((n,1))\n","    b = 0\n","    \n","    # Reshaping y.\n","    y = y.reshape(m,1)\n","    \n","    # Normalizing the inputs.\n","    x = normalize(X)\n","    \n","    # Empty list to store losses.\n","    losses = []\n","    \n","    # Training loop.\n","    for epoch in range(epochs):\n","        for i in range((m-1)//bs + 1):\n","            \n","            # Defining batches. SGD.\n","            start_i = i*bs\n","            end_i = start_i + bs\n","            xb = X[start_i:end_i]\n","            yb = y[start_i:end_i]\n","            \n","            # Calculating hypothesis/prediction.\n","            y_hat = sigmoid(np.dot(xb, w) + b)\n","            \n","            # Getting the gradients of loss w.r.t parameters.\n","            dw, db = gradients(xb, yb, y_hat)\n","            \n","            # Updating the parameters.\n","            w -= lr*dw\n","            b -= lr*db\n","        \n","        # Calculating loss and appending it in the list.\n","        l = loss(y, sigmoid(np.dot(X, w) + b))\n","        losses.append(l)\n","        \n","    # returning weights, bias and losses(List).\n","    return w, b, losses"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def predict(X):\n","    # X --> Input.\n","    \n","    # Normalizing the inputs.\n","    x = normalize(X)\n","    \n","    # Calculating predictions/y_hat.\n","    preds = sigmoid(np.dot(x, w) + b)\n","    \n","    # Empty List to store predictions.\n","    pred_class = [1 if i >= 0.5 else 0 for i in preds]\n","    \n","    return np.array(pred_class)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('test_tfidf_features.csv')\n","train_df = pd.read_csv('train_tfidf_features.csv')"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 5002 columns</p>\n","</div>"],"text/plain":["   id  label    0    1    2    3    4    5    6    7  ...  4990  4991  4992  \\\n","0   1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n","1   2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n","2   3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n","3   4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n","4   5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n","\n","   4993  4994  4995  4996  4997  4998  4999  \n","0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[5 rows x 5002 columns]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["X_train = train_df.drop(['label', 'id'], axis=1).values\n","y_train = train_df['label'].values"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Training \n","w, b, l = train(X_train, y_train, bs=100, epochs=10, lr=0.01)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (17184,5000) (17184,) ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y \u001b[38;5;241m==\u001b[39m y_hat) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m----> 5\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 2\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(y, y_hat)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(y, y_hat):\n\u001b[0;32m----> 2\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (17184,5000) (17184,) "]}],"source":["def accuracy(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / len(y)\n","    return accuracy\n","\n","accuracy(X, y_hat=predict(X))"]},{"cell_type":"markdown","metadata":{},"source":["### PCA Code ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","df = pd.read_csv('./path/to/train_tfidf_features.csv')\n","\n","# Separate features and labels\n","X = df.drop('label', axis=1)  # Replace 'label' with the actual label column name\n","y = df['label']\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Function to apply PCA\n","def apply_pca(n_components):\n","    pca = PCA(n_components=n_components)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","    return X_train_pca, X_test_pca\n","\n","# Apply PCA for different component sizes\n","components = [2000, 1000, 500, 100]\n","pca_results = {n: apply_pca(n) for n in components}"]},{"cell_type":"markdown","metadata":{},"source":["### Calculating macro f1 score ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","f1 = f1_score(y_test, y_pred, average='macro')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
