{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'unzip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!unzip 50-007-machine-learning-summer-2024.zip\n","\n","# run this cell to extract the data (but cannot commit after because file exceeds 100MB)"]},{"cell_type":"markdown","metadata":{},"source":["Your implementation should have the following functions:\n","\n","-- sigmoid(z): A function that takes in a Real Number input and returns an output value between 0 and 1.\n","\n","-- loss(y, y_hat): A loss function that allows us to minimize and determine the optimal parameters. The function takes in the actual labels y and the predicted labels y_hat, and returns the overall training loss. Note that you should be using the Log Loss function taught in class.\n","\n","-- gradients(X, y, y_hat): The Gradient Descent Algorithm to find the optimal values of our parameters. The function takes in the training feature X, actual labels y and the predicted labels y_hat, and returns the partial derivative of the Loss function with respect to weights (w) and bias (db).\n","\n","-- train(X, y, bs, epochs, lr): The training function for your model.\n","\n","-- predict(X): The prediction function where you can apply your validation and test sets."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def loss(y, y_hat):\n","    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))\n","    return loss"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def gradients(X, y, y_hat):\n","    \n","    # X --> Input.\n","    # y --> true/target value.\n","    # y_hat --> hypothesis/predictions.\n","    # w --> weights (parameter).\n","    # b --> bias (parameter).\n","    \n","    # m-> number of training examples.\n","    m = X.shape[0]\n","    \n","    # Gradient of loss w.r.t weights.\n","    dw = (1/m)*np.dot(X.T, (y_hat - y))\n","    \n","    # Gradient of loss w.r.t bias.\n","    db = (1/m)*np.sum((y_hat - y)) \n","    \n","    return dw, db\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def plot_decision_boundary(X, w, b):\n","    \n","    # X --> Inputs\n","    # w --> weights\n","    # b --> bias\n","    \n","    # The Line is y=mx+c\n","    # So, Equate mx+c = w.X + b\n","    # Solving we find m and c\n","    x1 = [min(X[:,0]), max(X[:,0])]\n","    m = -w[0]/w[1]\n","    c = -b/w[1]\n","    x2 = m*x1 + c\n","    \n","    # Plotting\n","    fig = plt.figure(figsize=(10,8))\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"g^\")\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n","    plt.xlim([-2, 2])\n","    plt.ylim([0, 2.2])\n","    plt.xlabel(\"feature 1\")\n","    plt.ylabel(\"feature 2\")\n","    plt.title('Decision Boundary')\n","    plt.plot(x1, x2, 'y-')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def normalize(X):\n","    \n","    # X --> Input.\n","    \n","    # m-> number of training examples\n","    # n-> number of features \n","    m, n = X.shape\n","    \n","    # Normalizing all the n features of X.\n","    for i in range(n):\n","        X = (X - X.mean(axis=0))/X.std(axis=0)\n","        \n","    return X"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def train(X, y, bs, epochs, lr):\n","    \n","    # X --> Input.\n","    # y --> true/target value.\n","    # bs --> Batch Size.\n","    # epochs --> Number of iterations.\n","    # lr --> Learning rate.\n","        \n","    # m-> number of training examples\n","    # n-> number of features \n","    m, n = X.shape\n","    \n","    # Initializing weights and bias to zeros.\n","    w = np.zeros((n,1))\n","    b = 0\n","    \n","    # Reshaping y.\n","    y = y.reshape(m,1)\n","    \n","    # Normalizing the inputs.\n","    x = normalize(X)\n","    \n","    # Empty list to store losses.\n","    losses = []\n","    \n","    # Training loop.\n","    for epoch in range(epochs):\n","        for i in range((m-1)//bs + 1):\n","            \n","            # Defining batches. SGD.\n","            start_i = i*bs\n","            end_i = start_i + bs\n","            xb = X[start_i:end_i]\n","            yb = y[start_i:end_i]\n","            \n","            # Calculating hypothesis/prediction.\n","            y_hat = sigmoid(np.dot(xb, w) + b)\n","            \n","            # Getting the gradients of loss w.r.t parameters.\n","            dw, db = gradients(xb, yb, y_hat)\n","            \n","            # Updating the parameters.\n","            w -= lr*dw\n","            b -= lr*db\n","        \n","        # Calculating loss and appending it in the list.\n","        l = loss(y, sigmoid(np.dot(X, w) + b))\n","        losses.append(l)\n","        \n","    # returning weights, bias and losses(List).\n","    return w, b, losses"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def predict(X):\n","    \n","    # X --> Input.\n","    \n","    # Normalizing the inputs.\n","    x = normalize(X)\n","    \n","    # Calculating presictions/y_hat.\n","    preds = sigmoid(np.dot(X, w) + b)\n","    \n","    # Empty List to store predictions.\n","    pred_class = []\n","    # if y_hat >= 0.5 --> round up to 1\n","    # if y_hat < 0.5 --> round up to 1\n","    pred_class = [1 if i > 0.5 else 0 for i in preds]\n","    \n","    return np.array(pred_class)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training \n","w, b, l = train(X, y, bs=100, epochs=1000, lr=0.01)\n","# Plotting Decision Boundary\n","plot_decision_boundary(X, w, b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def accuracy(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / len(y)\n","    return accuracy\n","\n","accuracy(X, y_hat=predict(X))"]},{"cell_type":"markdown","metadata":{},"source":["### PCA Code ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","df = pd.read_csv('./path/to/train_tfidf_features.csv')\n","\n","# Separate features and labels\n","X = df.drop('label', axis=1)  # Replace 'label' with the actual label column name\n","y = df['label']\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Function to apply PCA\n","def apply_pca(n_components):\n","    pca = PCA(n_components=n_components)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","    return X_train_pca, X_test_pca\n","\n","# Apply PCA for different component sizes\n","components = [2000, 1000, 500, 100]\n","pca_results = {n: apply_pca(n) for n in components}"]},{"cell_type":"markdown","metadata":{},"source":["### Calculating macro f1 score ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","f1 = f1_score(y_test, y_pred, average='macro')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
