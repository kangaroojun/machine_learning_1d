{"cells":[{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import f1_score\n","\n","# Define necessary functions (normalize, sigmoid, gradients, loss, train, predict)\n","\n","def normalize(X):\n","    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def gradients(X, y, y_hat):\n","    m = X.shape[0]\n","    dw = np.dot(X.T, (y_hat - y)) / m\n","    db = np.sum(y_hat - y) / m\n","    return dw, db\n","\n","def loss(y, y_hat):\n","    m = y.shape[0]\n","    return -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n","\n","def train(X, y, bs, epochs, lr):\n","    m, n = X.shape\n","    w = np.zeros((n, 1))\n","    b = 0\n","    y = y.reshape(m, 1)\n","    X = normalize(X)\n","    losses = []\n","    for epoch in range(epochs):\n","        for i in range((m-1) // bs + 1):\n","            start_i = i * bs\n","            end_i = start_i + bs\n","            xb = X[start_i:end_i]\n","            yb = y[start_i:end_i]\n","            y_hat = sigmoid(np.dot(xb, w) + b)\n","            dw, db = gradients(xb, yb, y_hat)\n","            w -= lr * dw\n","            b -= lr * db\n","        l = loss(y, sigmoid(np.dot(X, w) + b))\n","        losses.append(l)\n","    return w, b, losses\n","\n","def predict(X, w, b):\n","    x = normalize(X)\n","    preds = sigmoid(np.dot(x, w) + b)\n","    pred_class = [1 if i >= 0.5 else 0 for i in preds]\n","    return np.array(pred_class)\n","\n","def accuracy(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / len(y)\n","    return accuracy\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["df = pd.read_csv('train_tfidf_features.csv')\n","\n","X = df.drop(['label', 'id'], axis=1)\n","y = df['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def apply_pca(n_components):\n","    pca = PCA(n_components=n_components)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","    return X_train_pca, X_test_pca"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import f1_score\n","\n","# Load the dataset\n","df = pd.read_csv('./data/train_tfidf_features.csv')\n","\n","# Separate features and labels\n","X = df.drop('label', axis=1)  # Replace 'label_column' with the actual label column name\n","y = df['label']\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Function to apply PCA\n","def apply_pca(n_components):\n","    pca = PCA(n_components=n_components)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","    return X_train_pca, X_test_pca\n","\n","# Apply PCA for different component sizes\n","components = [2000, 1000, 500, 100]\n","pca_results = {n: apply_pca(n) for n in components}\n","\n","# Train KNN and evaluate\n","knn = KNeighborsClassifier(n_neighbors=2)\n","\n","def train_and_evaluate(X_train_pca, X_test_pca):\n","    knn.fit(X_train_pca, y_train)\n","    y_pred = knn.predict(X_test_pca)\n","    return y_pred\n","\n","results = {}\n","for n, (X_train_pca, X_test_pca) in pca_results.items():\n","    y_pred = train_and_evaluate(X_train_pca, X_test_pca)\n","    f1 = f1_score(y_test, y_pred, average='macro')\n","    results[n] = f1\n","\n","# Print results\n","for n, f1 in results.items():\n","    print(f\"PCA Components: {n}, Macro F1 Score: {f1}\")\n","\n","# Save predictions for Kaggle submission (example for 100 components)\n","y_pred_100 = train_and_evaluate(*pca_results[100])\n","submission = pd.DataFrame({'Id': X_test.index, 'Prediction': y_pred_100})\n","submission.to_csv('submission_100_components.csv', index=False)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/4b/z3dqqqrd24x3ph1yv37zzk3c0000gn/T/ipykernel_78823/3199313973.py:8: RuntimeWarning: invalid value encountered in divide\n","  return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n"]},{"name":"stdout","output_type":"stream","text":["Training Accuracy: 0.6187732774674115\n"]}],"source":["# Assuming train_df and test_df are your DataFrames\n","train_df = pd.read_csv('train_tfidf_features.csv')\n","test_df = pd.read_csv('test_tfidf_features.csv')\n","\n","# Prepare training data\n","X_train = train_df.drop(['label', 'id'], axis=1).values\n","y_train = train_df['label'].values\n","\n","# Call the training function\n","w, b, l = train(X_train, y_train, bs=100, epochs=10, lr=0.01)\n","\n","# Predictions and accuracy on training data\n","y_train_pred = predict(X_train, w, b)\n","print(f\"Training Accuracy: {accuracy(y_train, y_train_pred)}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 0.3822482654491858\n"]}],"source":["# Calculate F1 score\n","f1 = f1_score(y_train, y_train_pred, average='macro')\n","print(f\"F1 Score: {f1}\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"\"['label'] not found in axis\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predictions and accuracy on test data\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n","\u001b[0;31mKeyError\u001b[0m: \"['label'] not found in axis\""]}],"source":["# Prepare test data\n","X_test = test_df.drop(['label', 'id'], axis=1).values\n","y_test = test_df['label'].values\n","\n","# Predictions and accuracy on test data\n","y_test_pred = predict(X_test, w, b)\n","print(f\"Test Accuracy: {accuracy(y_test, y_test_pred)}\")\n","\n","# Calculate F1 score\n","f1 = f1_score(y_test, y_test_pred, average='macro')\n","print(f\"F1 Score: {f1}\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: xgboost in /Users/kangjunong/Library/Python/3.9/lib/python/site-packages (2.1.0)\n","Requirement already satisfied: numpy in /Users/kangjunong/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.25.2)\n","Requirement already satisfied: scipy in /Users/kangjunong/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.11.4)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting lightgbm\n","  Downloading lightgbm-4.4.0.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/kangjunong/Library/Python/3.9/lib/python/site-packages (from lightgbm) (1.25.2)\n","Requirement already satisfied: scipy in /Users/kangjunong/Library/Python/3.9/lib/python/site-packages (from lightgbm) (1.11.4)\n","Building wheels for collected packages: lightgbm\n","  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25l/"]}],"source":["!pip install xgboost\n","!pip install lightgbm\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'lightgbm'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_tfidf_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.decomposition import PCA, KernelPCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import f1_score\n","from xgboost import XGBClassifier\n","from joblib import Parallel, delayed\n","import lightgbm as lgb\n","\n","# Load the dataset\n","df = pd.read_csv('train_tfidf_features.csv')\n","\n","# Separate features and labels\n","X = df.drop('label', axis=1)  # Replace 'label' with the actual label column name\n","y = df['label']\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Function to apply PCA or Kernel PCA\n","def apply_pca(X, n_components, kernel=None):\n","    if kernel:\n","        pca = KernelPCA(n_components=n_components, kernel=kernel)\n","    else:\n","        pca = PCA(n_components=n_components)\n","    X_pca = pca.fit_transform(X)\n","    return X_pca\n","\n","# List of PCA components to evaluate\n","components = [2000, 1000, 500, 100]\n","\n","# Function to perform grid search for KNN\n","def grid_search_knn(X_pca, y):\n","    param_grid = {'n_neighbors': [1, 2, 3, 5, 7, 10]}\n","    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='f1_macro', cv=5)\n","    grid_search.fit(X_pca, y)\n","    return grid_search.best_estimator_\n","\n","# Function to evaluate a model with k-fold cross-validation\n","def evaluate_model_kfold(model, X_pca, y, k=5):\n","    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","    f1_scores = []\n","    for train_index, test_index in kf.split(X_pca):\n","        X_train, X_test = X_pca[train_index], X_pca[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n","    return np.mean(f1_scores)\n","\n","# Function to train and evaluate models\n","def train_and_evaluate(X, y, n_components, kernel=None, k=5):\n","    X_pca = apply_pca(X, n_components, kernel)\n","    \n","    # KNN\n","    best_knn = grid_search_knn(X_pca, y)\n","    knn_f1 = evaluate_model_kfold(best_knn, X_pca, y, k)\n","    \n","    # Random Forest\n","    rf_f1 = evaluate_model_kfold(RandomForestClassifier(), X_pca, y, k)\n","    \n","    # SVM\n","    svm_f1 = evaluate_model_kfold(SVC(), X_pca, y, k)\n","    \n","    # XGBoost\n","    xgb_f1 = evaluate_model_kfold(XGBClassifier(), X_pca, y, k)\n","    \n","    # LightGBM\n","    lgb_f1 = evaluate_model_kfold(lgb.LGBMClassifier(), X_pca, y, k)\n","    \n","    return n_components, knn_f1, rf_f1, svm_f1, xgb_f1, lgb_f1\n","\n","# Parallel computation for all components\n","results = Parallel(n_jobs=-1)(delayed(train_and_evaluate)(X, y, n) for n in components)\n","\n","# Print results\n","for res in results:\n","    n_components, knn_f1, rf_f1, svm_f1, xgb_f1, lgb_f1 = res\n","    print(f\"PCA Components: {n_components}\")\n","    print(f\"KNN Macro F1 Score: {knn_f1}\")\n","    print(f\"Random Forest Macro F1 Score: {rf_f1}\")\n","    print(f\"SVM Macro F1 Score: {svm_f1}\")\n","    print(f\"XGBoost Macro F1 Score: {xgb_f1}\")\n","    print(f\"LightGBM Macro F1 Score: {lgb_f1}\")\n","\n","# Save best KNN predictions for Kaggle submission (example for 100 components)\n","X_pca = apply_pca(X, 100)\n","best_knn = grid_search_knn(X_pca, y)\n","y_pred_100 = cross_val_predict(best_knn, X_pca, y, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n","submission = pd.DataFrame({'Id': np.arange(len(y)), 'Prediction': y_pred_100})\n","submission.to_csv('submission_100_components.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
