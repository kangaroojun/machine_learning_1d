{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 ###\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 0.6908383573948907, Beta [-1.15761909e-03  1.92035294e-05 -8.05249720e-05  2.22213018e-05\n",
      "  2.93724623e-05]\n",
      "Iteration 50: Cost 0.6003190302308883, Beta [-0.05551384  0.00064915 -0.00366712  0.00099683  0.00128311]\n",
      "Iteration 100: Cost 0.5424007585248339, Beta [-0.10365702  0.00078116 -0.00655588  0.00174863  0.00219551]\n",
      "Final Beta: [-1.24851828e-01  7.20731156e-04 -7.75337259e-03 ... -3.21755855e-03\n",
      " -7.34490599e-05  3.77841653e-03]\n",
      "Final Cost: 0.5215186010762247\n",
      "[[2380  846]\n",
      " [ 716 1213]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6806256320368889)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def split_data(df_feature, df_target, random_state=None, test_size=0.5):\n",
    "    indexes = df_feature.index.to_numpy()\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    test_index = np.random.choice(indexes, int(len(indexes) * test_size), replace=False)\n",
    "    train_index = np.setdiff1d(indexes, test_index)\n",
    "    \n",
    "    df_feature_train = df_feature.loc[train_index, :]\n",
    "    df_feature_test = df_feature.loc[test_index, :]\n",
    "    df_target_train = df_target.loc[train_index, :]\n",
    "    df_target_test = df_target.loc[test_index, :]\n",
    "    \n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    "\n",
    "def normalize_z(dfin, columns_means=None, columns_stds=None):\n",
    "    if columns_means is None:\n",
    "        columns_means = dfin.mean(axis=0)\n",
    "    if columns_stds is None:\n",
    "        columns_stds = dfin.std(axis=0)\n",
    "    \n",
    "    # Prevent division by zero\n",
    "    columns_stds = columns_stds.replace(0, 1e-5)\n",
    "    \n",
    "    dfout = (dfin - columns_means) / columns_stds\n",
    "    return dfout, columns_means, columns_stds\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    if isinstance(df_feature, pd.DataFrame):\n",
    "        np_feature = df_feature.to_numpy()\n",
    "    else:\n",
    "        np_feature = df_feature\n",
    "    X = np.hstack((np.ones((np_feature.shape[0], 1)), np_feature))\n",
    "    return X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    if isinstance(df_target, pd.DataFrame):\n",
    "        np_target = df_target.to_numpy()\n",
    "    else:\n",
    "        np_target = df_target\n",
    "    return np_target.ravel()\n",
    "\n",
    "def calc_logreg(X, beta):\n",
    "    z = np.dot(X, beta)\n",
    "    p_x = 1 / (1 + np.exp(-z))\n",
    "    return p_x\n",
    "\n",
    "def compute_cost_linreg(beta, X, y):\n",
    "    epsilon = 1e-5\n",
    "    pred = np.clip(calc_logreg(X, beta), epsilon, 1 - epsilon)\n",
    "    error = np.where(y == 1, np.log(pred), np.log(1 - pred))\n",
    "    J = -np.mean(error)\n",
    "    return J\n",
    "\n",
    "def gradient_descent_logreg(X, y, beta, alpha, num_iters):\n",
    "    m = y.shape[0]\n",
    "    J_storage = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        pred = calc_logreg(X, beta)\n",
    "        error = pred - y\n",
    "        gradient = np.dot(X.T, error) / m\n",
    "        beta -= alpha * gradient\n",
    "        J_storage[i] = compute_cost_linreg(beta, X, y)\n",
    "        \n",
    "        # Debugging: print intermediate values\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}: Cost {J_storage[i]}, Beta {beta[:5]}\")  # Print the first 5 beta values for readability\n",
    "        \n",
    "    return beta, J_storage\n",
    "\n",
    "def predict_norm(X, beta):\n",
    "    probabilities = calc_logreg(X, beta)\n",
    "    return np.where(probabilities >= 0.5, 1, 0)\n",
    "\n",
    "def predict_logreg(df_feature, beta, means=None, stds=None):\n",
    "    df_feature, means, stds = normalize_z(df_feature, means, stds)\n",
    "    X = prepare_feature(df_feature)\n",
    "    return predict_norm(X, beta)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "df_feature = df.drop('label', axis=1)\n",
    "df_feature = df_feature.drop('id', axis=1)\n",
    "target = pd.DataFrame(df['label'])\n",
    "\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, target, random_state=42, test_size=0.3)\n",
    "\n",
    "df_feature_train_norm, means, stds = normalize_z(df_feature_train)\n",
    "\n",
    "X = prepare_feature(df_feature_train_norm)\n",
    "y = prepare_target(df_target_train)\n",
    "\n",
    "beta = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 125\n",
    "beta, J_storage = gradient_descent_logreg(X, y, beta, alpha, num_iters)\n",
    "\n",
    "predictions = predict_logreg(df_feature_test, beta, means, stds)\n",
    "\n",
    "# Print final beta and cost\n",
    "print(f\"Final Beta: {beta}\")\n",
    "print(f\"Final Cost: {J_storage[-1]}\")\n",
    "\n",
    "cm = confusion_matrix(df_target_test, predictions)\n",
    "print(cm)\n",
    "\n",
    "f1_score(df_target_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro F1 Score for our Logistic Regression model: 0.68062..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKLearn implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\banno\\OneDrive - Singapore University of Technology and Design\\Desktop\\machine_learning_1d\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6804009001282998)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "df_feature = df.drop('label', axis=1)\n",
    "df_feature = df_feature.drop('id', axis=1)\n",
    "target = pd.DataFrame(df['label'])\n",
    "\n",
    "\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, target, random_state=42, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df_feature_train, df_target_train)\n",
    "predictions = model.predict(df_feature_test)\n",
    "\n",
    "#f1 score\n",
    "f1_score(df_target_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro F1 Score for SKLearn implementation: 0.68040..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 ###\n",
    "PCA and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA:\n",
    "\n",
    "PCA in sklearn takes in these arguments:\n",
    "\n",
    "1. n_components: int/float\n",
    "    - Number of components to keep. By deafult is all components\n",
    "    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=42, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_feature_train_scaled = scaler.fit_transform(df_feature_train)\n",
    "df_feature_test_scaled = scaler.transform(df_feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:\n",
    "\n",
    "Most imporatantly, KNeighborsClassifier in sklearn takes in these arguments: \n",
    "1. n_neighours: int\n",
    "    - the number of neighbours that we will be comparing to for us to determine how to classify the identified point\n",
    "2. weights: ['uniform', 'distance']\n",
    "    - uniform: All points in each neighborhood are weighted equally\n",
    "    - distance: Weigh points by the inverse of their distance. --> Closer neighbors of a query point will have greater influence than neighbors which are further away\n",
    "3. metrics: str\n",
    "    - metric to use for distance computation. Default is minkowski\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = [2000, 1000, 500, 100]\n",
    "f1_dict = {'uniform, minkowski': [0, 0, 0, 0],\n",
    "           'uniform, euclidean': [0, 0, 0, 0],\n",
    "           'uniform, manhattan': [0, 0, 0, 0],\n",
    "           'distance, minkowski': [0, 0, 0, 0],\n",
    "           'distance, euclidean': [0, 0, 0, 0],\n",
    "           'distance, manhattan': [0, 0, 0, 0]}\n",
    "\n",
    "for i, n in enumerate(n_components):\n",
    "    pca = PCA(n_components=n)\n",
    "    df_feature_train_pca = pca.fit_transform(df_feature_train_scaled)\n",
    "    df_feature_test_pca = pca.transform(df_feature_test_scaled)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', metric='minkowski')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: uniform, metrics: minkowski, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['uniform, minkowski'][i] = macro_f1\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', metric='euclidean')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: uniform, metrics: euclidean, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['uniform, euclidean'][i] = macro_f1\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', metric='manhattan')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: uniform, metrics: manhattan, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['uniform, manhattan'][i] = macro_f1\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='distance', metric='minkowski')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: distance, metrics: minkowski, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['distance, minkowski'][i] = macro_f1\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='distance', metric='euclidean')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: distance, metrics: euclidean, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['distance, euclidean'][i] = macro_f1\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=2, weights='distance', metric='manhattan')\n",
    "    knn.fit(df_feature_train_pca, df_target_train)\n",
    "    y_pred = knn.predict(df_feature_test_pca)\n",
    "    \n",
    "    macro_f1 = f1_score(df_target_test, y_pred, average='macro')\n",
    "    print(f\"PCA Components: {n}, weight: distance, metrics: manhattan, Macro F1 Score: {macro_f1}\")\n",
    "    f1_dict['distance, manhattan'][i] = macro_f1\n",
    "    # submission = pd.DataFrame({'Id': np.arange(len(y_pred)), 'Predicted': y_pred})\n",
    "    # submission.to_csv(f'./knn_submissions/knn_pca_{n}_components_submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Components: 2000, weight: uniform, metrics: minkowski, Macro F1 Score: 0.5501698573624343\n",
    "\n",
    "PCA Components: 2000, weight: uniform, metrics: euclidean, Macro F1 Score: 0.5501698573624343\n",
    "\n",
    "PCA Components: 2000, weight: uniform, metrics: manhattan, Macro F1 Score: 0.5524122193964222\n",
    "\n",
    "PCA Components: 2000, weight: distance, metrics: minkowski, Macro F1 Score: 0.5392491953449301\n",
    "\n",
    "PCA Components: 2000, weight: distance, metrics: euclidean, Macro F1 Score: 0.5392491953449301\n",
    "\n",
    "PCA Components: 2000, weight: distance, metrics: manhattan, Macro F1 Score: 0.5435481359602898\n",
    "\n",
    "\n",
    "PCA Components: 1000, weight: uniform, metrics: minkowski, Macro F1 Score: 0.5509358763631034\n",
    "\n",
    "PCA Components: 1000, weight: uniform, metrics: euclidean, Macro F1 Score: 0.5509358763631034\n",
    "\n",
    "PCA Components: 1000, weight: uniform, metrics: manhattan, Macro F1 Score: 0.5508372124746364\n",
    "\n",
    "PCA Components: 1000, weight: distance, metrics: minkowski, Macro F1 Score: 0.5501770344051041\n",
    "\n",
    "PCA Components: 1000, weight: distance, metrics: euclidean, Macro F1 Score: 0.5501770344051041\n",
    "\n",
    "PCA Components: 1000, weight: distance, metrics: manhattan, Macro F1 Score: 0.5495024023827315\n",
    "\n",
    "\n",
    "PCA Components: 500, weight: uniform, metrics: minkowski, Macro F1 Score: 0.5537058356694703\n",
    "\n",
    "PCA Components: 500, weight: uniform, metrics: euclidean, Macro F1 Score: 0.5537058356694703\n",
    "\n",
    "PCA Components: 500, weight: uniform, metrics: manhattan, Macro F1 Score: 0.5517738757177308\n",
    "\n",
    "PCA Components: 500, weight: distance, metrics: minkowski, Macro F1 Score: 0.5605568943455942\n",
    "\n",
    "PCA Components: 500, weight: distance, metrics: euclidean, Macro F1 Score: 0.5605568943455942\n",
    "\n",
    "PCA Components: 500, weight: distance, metrics: manhattan, Macro F1 Score: 0.560207813194894\n",
    "\n",
    "\n",
    "PCA Components: 100, weight: uniform, metrics: minkowski, Macro F1 Score: 0.5486275090215184\n",
    "\n",
    "PCA Components: 100, weight: uniform, metrics: euclidean, Macro F1 Score: 0.5486275090215184\n",
    "\n",
    "PCA Components: 100, weight: uniform, metrics: manhattan, Macro F1 Score: 0.5452027672093189\n",
    "\n",
    "PCA Components: 100, weight: distance, metrics: minkowski, Macro F1 Score: 0.5817419006772417\n",
    "\n",
    "PCA Components: 100, weight: distance, metrics: euclidean, Macro F1 Score: 0.5817419006772417\n",
    "\n",
    "PCA Components: 100, weight: distance, metrics: manhattan, Macro F1 Score: 0.5765390074323072\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniform, minkowski': np.float64(0.45174245826512327),\n",
       " 'uniform, euclidean': np.float64(0.45174245826512327),\n",
       " 'uniform, manhattan': np.float64(0.43015677039496125),\n",
       " 'distance, minkowski': np.float64(0.44654508929493897),\n",
       " 'distance, euclidean': np.float64(0.44654508929493897),\n",
       " 'distance, manhattan': np.float64(0.4248016097529777)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA components None\n",
    "\n",
    "{'uniform, minkowski': np.float64(0.45174245826512327),\n",
    " 'uniform, euclidean': np.float64(0.45174245826512327),\n",
    " 'uniform, manhattan': np.float64(0.43015677039496125),\n",
    " 'distance, minkowski': np.float64(0.44654508929493897),\n",
    " 'distance, euclidean': np.float64(0.44654508929493897),\n",
    " 'distance, manhattan': np.float64(0.4248016097529777)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniform, minkowski': [np.float64(0.5501698573624343),\n",
       "  np.float64(0.5509358763631034),\n",
       "  np.float64(0.5537058356694703),\n",
       "  np.float64(0.5486275090215184)],\n",
       " 'uniform, euclidean': [np.float64(0.5501698573624343),\n",
       "  np.float64(0.5509358763631034),\n",
       "  np.float64(0.5537058356694703),\n",
       "  np.float64(0.5486275090215184)],\n",
       " 'uniform, manhattan': [np.float64(0.5524122193964222),\n",
       "  np.float64(0.5508372124746364),\n",
       "  np.float64(0.5517738757177308),\n",
       "  np.float64(0.5452027672093189)],\n",
       " 'distance, minkowski': [np.float64(0.5392491953449301),\n",
       "  np.float64(0.5501770344051041),\n",
       "  np.float64(0.5605568943455942),\n",
       "  np.float64(0.5817419006772417)],\n",
       " 'distance, euclidean': [np.float64(0.5392491953449301),\n",
       "  np.float64(0.5501770344051041),\n",
       "  np.float64(0.5605568943455942),\n",
       "  np.float64(0.5817419006772417)],\n",
       " 'distance, manhattan': [np.float64(0.5435481359602898),\n",
       "  np.float64(0.5495024023827315),\n",
       "  np.float64(0.560207813194894),\n",
       "  np.float64(0.5765390074323072)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = 2000, 1000, 500, 100\n",
    "\n",
    "{'uniform, minkowski': [np.float64(0.5501698573624343),\n",
    "                        np.float64(0.5509358763631034),\n",
    "                        np.float64(0.5537058356694703),\n",
    "                        np.float64(0.5486275090215184)],\n",
    "\n",
    "                        \n",
    " 'uniform, euclidean': [np.float64(0.5501698573624343),\n",
    "                        np.float64(0.5509358763631034),\n",
    "                        np.float64(0.5537058356694703),\n",
    "                        np.float64(0.5486275090215184)],\n",
    "\n",
    "\n",
    " 'uniform, manhattan': [np.float64(0.5524122193964222),\n",
    "                        np.float64(0.5508372124746364),\n",
    "                        np.float64(0.5517738757177308),\n",
    "                        np.float64(0.5452027672093189)],\n",
    "\n",
    "\n",
    " 'distance, minkowski': [np.float64(0.5392491953449301),\n",
    "                        np.float64(0.5501770344051041),\n",
    "                        np.float64(0.5605568943455942),\n",
    "                        np.float64(0.5817419006772417)],\n",
    "\n",
    "\n",
    " 'distance, euclidean': [np.float64(0.5392491953449301),\n",
    "                        np.float64(0.5501770344051041),\n",
    "                        np.float64(0.5605568943455942),\n",
    "                        np.float64(0.5817419006772417)],\n",
    "\n",
    "\n",
    " 'distance, manhattan': [np.float64(0.5435481359602898),\n",
    "                        np.float64(0.5495024023827315),\n",
    "                        np.float64(0.560207813194894),\n",
    "                        np.float64(0.5765390074323072)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 (Bernoulli Naive Bayes) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78      2127\n",
      "           1       0.65      0.57      0.61      1310\n",
      "\n",
      "    accuracy                           0.72      3437\n",
      "   macro avg       0.71      0.69      0.70      3437\n",
      "weighted avg       0.72      0.72      0.72      3437\n",
      "\n",
      "Macro F1 Score: 0.6977701451503642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "# Prepare the data\n",
    "df_feature = df.drop(['label', 'id'], axis=1)\n",
    "df_target = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feature, df_target, test_size=0.2, random_state=42, stratify=df_target)\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes model\n",
    "bnb = BernoulliNB(alpha=1,  binarize=0.1)\n",
    "\n",
    "# alpha is the smoothing parameter that is used to handle 0 probabilities in the dataset, which can occur if a feature has never been observed with a particular class label during training. \n",
    "# This can lead to a probability of 0 for that feature, which can cause the entire probability calculation to be 0. \n",
    "# To avoid this, we add a small value to the probability calculation to ensure that the probability is never 0. In our case, we added 1 to the count of each feature-class combination before cauculating the probability.\n",
    "# alpha=1 because a higher value of alpha means more smoothing, which can help to avoid overfitting. Useful in highly imbalanced datasets or when we are dealing with sparse features.\n",
    "\n",
    "# binarize is the threshold value used to binarize the input features. If a feature value is greater than the threshold, it is set to 1, otherwise it is set to 0.\n",
    "\n",
    "# Fit the model\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Print classification report and macro F1 score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Macro F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 (Multinomial Naive Bayes) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.90      0.80      2127\n",
      "           1       0.73      0.44      0.55      1310\n",
      "\n",
      "    accuracy                           0.72      3437\n",
      "   macro avg       0.72      0.67      0.67      3437\n",
      "weighted avg       0.72      0.72      0.70      3437\n",
      "\n",
      "Macro F1 Score: 0.6739739021610032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "# Prepare the data\n",
    "df_feature = df.drop(['label', 'id'], axis=1)\n",
    "df_target = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feature, df_target, test_size=0.2, random_state=42, stratify=df_target)\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes model\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "\n",
    "# Fit the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Print classification report and macro F1 score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Macro F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we believed that multinomial naive bayes would be the best model for our use case. The SKLearn documentation mentions: \n",
    "\n",
    "\"The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "\n",
    "Which gave us confidence that this would be one of the better models. However, when we actually tried using it, we were disappointed by its performance and surprised to find that Bernoulli Naive Bayes worked better instead.\n",
    "\n",
    "This is probably due to the fact that the dataset have very sparse features. We believe that the simplicity of binary features led to better performance by focusing attention on the presence of features across the corpus rather than their varied weights.\n",
    "\n",
    "It could also be considered that by reducing the complexity of the feature space from many different unique tf-idf scores to just 1 or 0, it helped prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 (RandomForestClassifier) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\banno\\AppData\\Local\\Temp\\ipykernel_16040\\3600468840.py:19: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  return np_target.ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78      3190\n",
      "           1       0.64      0.56      0.60      1966\n",
      "\n",
      "    accuracy                           0.71      5156\n",
      "   macro avg       0.70      0.68      0.69      5156\n",
      "weighted avg       0.71      0.71      0.71      5156\n",
      "\n",
      "Test macro F1 score: 0.69\n",
      "Test macro F1 score: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, recall_score, make_scorer, f1_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    if isinstance(df_feature, pd.DataFrame):\n",
    "        np_feature = df_feature.to_numpy()\n",
    "    else:\n",
    "        np_feature = df_feature\n",
    "    return np_feature\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    if isinstance(df_target, pd.DataFrame):\n",
    "        np_target = df_target.to_numpy()\n",
    "    else:\n",
    "        np_target = df_target\n",
    "    return np_target.ravel()\n",
    "\n",
    "def split_data(X, y, test_size=0.3, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "# Prepare the data\n",
    "df_feature = df.drop(['label', 'id'], axis=1)\n",
    "df_target = df['label']\n",
    "\n",
    "prepared_feature = prepare_feature(df_feature)\n",
    "prepared_target = prepare_target(df_target)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = split_data(prepared_feature, prepared_target, random_state=42, test_size=0.3)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=42, bootstrap=False)\n",
    "\n",
    "# n_estimators is the number of trees in the forest. A higher number of trees can lead to better performance, but it can also increase the training time.\n",
    "# bootstrap is a boolean parameter that specifies whether to use bootstrapping when building the trees. If bootstrap is set to False, the entire training dataset is used to build each tree.\n",
    "# random_state is the seed used by the random number generator. Since bootstrap is set to False, the random_state parameter controls the randomness in the selection of features for determining the best splits at each node of each tree, ensuring consistent feature subsets across different runs of the model when the same seed is used. It does not influence the selection of data samples since the entire dataset is used for each tree\n",
    "\n",
    "# Tested rf with different hyperparemeters:\n",
    "#     'n_estimators': [100, 200, 300, 400],\n",
    "#     'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#     'bootstrap': [True, False]\n",
    "# Found the best hyperparameters to be: n_estimators=400, max_depth=None, min_samples_leaf=1, max_features='sqrt', bootstrap=False\n",
    "\n",
    "# max_features='sqrt' means the number of features to consider when looking for the best split is the square root of the total number of features. \n",
    "# For each node where a split decision needs to be made, the algorithm randomly selects a new subset of features equal to the square root of the total number of features. This subset could be different from those considered in previous or subsequent splits within the same tree\n",
    "# max_depth=None means the maximum depth of the tree is not limited. The tree will continue to grow until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "# min_samples_leaf=1 means the minimum number of samples required to be at a leaf node is 1. This means that each leaf node will have at least one sample\n",
    "\n",
    "# The best hyperparameters were found using RandomizedSearchCV and we realised that these parameters are the default parameters. Only n_estimators was changed to 400 and bootstrap was set to False.\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "test_macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "print(f\"Test macro F1 score: {test_macro_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the macro f1 score was 0.69 here, in the public leaderboard, the score was slightly below 0.71. This was the first model that helped us get past the public blue line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 (LightGBM) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 (XGBoost) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    10633\n",
      "1     6551\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    61.877328\n",
      "1    38.122672\n",
      "Name: count, dtype: float64\n",
      "Count of '1' in label: 6551\n",
      "Percentage of '1' in label: 38.12%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "\n",
    "# Prepare the data\n",
    "df_feature = df.drop(['label', 'id'], axis=1)\n",
    "df_target = df['label']\n",
    "\n",
    "# Getting value counts for all unique values in the label column\n",
    "class_counts = df_target.value_counts()\n",
    "print(class_counts)\n",
    "print(class_counts / len(df_target) * 100)  # Printing the percentage representation of each class\n",
    "\n",
    "# Specifically checking entries where the label is 1\n",
    "count_label_1 = (df_target == 1).sum()  # Counts the number of times '1' appears in the label column\n",
    "percentage_label_1 = count_label_1 / len(df_target) * 100  # Calculates the percentage of '1' labels\n",
    "print(f\"Count of '1' in label: {count_label_1}\")\n",
    "print(f\"Percentage of '1' in label: {percentage_label_1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ the SMOTE resampling technique to reach a balance between the 0 and 1 classes.\n",
    "\n",
    "Originally, when we did XGBoost without the resampling technique, the performance was not the best. After introducing the resampling technique, it produced better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with PCA and XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA()), \n",
    "    ('classifier', xgb.XGBClassifier(objective='binary:logistic', learning_rate=0.1))\n",
    "])\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': [2000, 1000, 500, 100],  # PCA components\n",
    "    'classifier__n_estimators': [50, 100, 200, 300],  # Number of trees in XGBoost, equivalent to the number of boosting rounds\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7]  # Maximum depth of the each tree in XGBoost\n",
    "}\n",
    "\n",
    "# Define a scorer for F1 score\n",
    "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=macro_f1_scorer, cv=3, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(df_feature_train_scaled, df_target_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation macro F1 score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(df_feature_test_scaled)\n",
    "test_macro_f1 = f1_score(df_target_test, predictions, average='macro')\n",
    "print(f\"Test macro F1 score: {test_macro_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "prepared_features = prepare_feature(df_feature)\n",
    "prepared_target = prepare_target(df_target)\n",
    "\n",
    "# Split the data\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(prepared_features, prepared_target, random_state=42, test_size=0.3)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "# SMOTE identifies the minority class that needs oversampling to reach a balance between the classes\n",
    "# It then selects a random sample from the minority class and computes the k-nearest neighbors for this sample\n",
    "# A random neighbor is selected and a random amount between 0 and 1 is chosen\n",
    "# A new sample is created by adding the random amount to the original sample\n",
    "smote = SMOTE(random_state=42)\n",
    "df_feature_train_res, df_target_train_res = smote.fit_resample(df_feature_train, df_target_train)\n",
    "\n",
    "# pca = PCA(n_components=4000)\n",
    "# df_feature_train_pca = pca.fit_transform(df_feature_train_res)\n",
    "# df_feature_test_pca = pca.transform(df_feature_test)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_feature_train_scaled = scaler.fit_transform(df_feature_train_res)\n",
    "df_feature_test_scaled = scaler.transform(df_feature_test)\n",
    "\n",
    "param_distributions = {\n",
    "    'max_depth': [5, 6, 7, 8],\n",
    "    'scale_pos_weight': [1, 2],  # ratio of the number of negative class to the positive class. Setting it to 1 means no scaling is applied, which is appropriate when classes are balanced. When classes are imbalanced, this parameter can be set to a value that compensates for the imbalance by giving more weight to the minority class\n",
    "    'min_child_weight': [1, 2],  # corresponds to the minimum number of instances needed to make a further partition on a leaf node. Higher values prevent the model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],  # node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # Defines the fraction of samples (rows) to be randomly sampled for each tree. Sampling is done without replacement.\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]  # specifies the fraction of features (columns) to be randomly sampled for each tree\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, n_estimators=200, learning_rate=0.1)\n",
    "macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
    "random_search = RandomizedSearchCV(model, param_distributions, n_iter=100, scoring=macro_f1_scorer, cv=3, verbose=1, random_state=42)\n",
    "random_search.fit(df_feature_train_scaled, df_target_train_res)\n",
    "\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "predictions = best_model.predict(df_feature_test_scaled)\n",
    "test_macro_f1 = f1_score(df_target_test, predictions, average='macro')\n",
    "print(f\"Test macro F1 score: {test_macro_f1:.2f}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(classification_report(df_target_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters from the previous line of code gave us this model\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, n_estimators=200, learning_rate=0.1, subsample= 0.7, scale_pos_weight= 1, min_child_weight= 1, max_depth= 8, gamma= 0.4, colsample_bytree= 1.0)\n",
    "\n",
    "# f1 score is 0.71 on the train test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6 (Bagging Classifier) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7 (AdaBoost) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8 (Gaussian NB) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model (Ensemble Model) ####\n",
    "Bernouli Naive Bayes, LightGBM, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our final model with the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/train_tfidf_features.csv')\n",
    "df_target = df['label']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "X = df.drop(['label', 'id'], axis=1)\n",
    "y = df['label']\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_target)\n",
    "\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes model\n",
    "bnb = BernoulliNB(alpha=1,  binarize=0.1)\n",
    "rf = RandomForestClassifier(n_estimators=375, random_state=42, bootstrap=True)\n",
    "lgbm_params = {\n",
    "    'colsample_bytree': 0.9918482913772408,\n",
    "    'learning_rate': 0.1483610742895996,\n",
    "    'max_depth': 8,\n",
    "    'n_estimators': 466,\n",
    "    'num_leaves': 89,\n",
    "    'subsample': 0.9890099850393909\n",
    "}\n",
    "lgbm = LGBMClassifier(objective='binary', random_state=42, class_weight='balanced', **lgbm_params)\n",
    "\n",
    "# Define the ensemble model using VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('bnb', bnb), ('rf', rf), ('lgbm', lgbm)], voting='soft')\n",
    "\n",
    "# Predict on the validation set\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "# Print classification report and macro F1 score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Macro F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining the model on the entire train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test_tfidf_features.csv')\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "# df_test_prepared = prepare_feature(df_test)\n",
    "ensemble_submission = VotingClassifier(estimators=[('bnb', bnb), ('rf', rf), ('lgbm', lgbm)], voting='soft')\n",
    "ensemble_submission.fit(X, y)\n",
    "predictions = ensemble_submission.predict(df_test)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['label'])\n",
    "test_id = pd.read_csv('./data/test_tfidf_features.csv')['id']\n",
    "submission_df = pd.concat([test_id, predictions_df\n",
    "                           ], axis=1)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
